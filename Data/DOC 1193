Title : Torch (machine learning)
Text : {{Infobox software
| name                   = Torch
| logo                   = [[File:Torch 2014 logo.png|Torch logo]]
| screenshot             =
| caption                =
| collapsible            =
| author                 = Ronan Collobert, Koray Kavukcuoglu, Clement Farabet
| developer              =
| released               = {{Start date and age|df=yes|2002|October}}<ref name="torch-modular">{{cite web|title=Torch: a modular machine learning software library|url=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=CBB0C8A5FE34F6D6DAFF997F6B6A205A?doi=10.1.1.8.9850&rep=rep1&type=pdf|date=30 October 2002|accessdate=24 April 2014}}</ref>
| latest release version = 7.0
| latest release date    = {{Start date and age|2017|02|27}}<ref>{{cite web |first=Ronan|last=Collobert |title=Torch7 |url=http://torch.ch/ |website=[[GitHub]]}}</ref>
| latest preview version = 
| latest preview date    = 
| programming language   = [[Lua (programming language)|Lua]], [[LuaJIT]], [[C (programming language)|C]], [[CUDA]] and [[C++]]
| operating system       = [[Linux]], [[Android (operating system)|Android]], [[Mac OS X]], [[iOS]]
| platform               =
| size                   =
| language               =
| status                 =
| genre                  = Library for [[machine learning]] and [[deep learning]]
| license                = [[BSD License]]
| website                = {{url|http://torch.ch/}}
}}
'''Torch''' is an [[open-source software|open-source]] [[machine learning]] library, 
a [[scientific computing]] framework, and a [[script language]] based on the [[Lua (programming language)|Lua]] programming language.<ref name="nips">{{cite journal
|author1=Ronan Collobert
|author2=Koray Kavukcuoglu
|author3=Clement Farabet
|title=Torch7: A Matlab-like Environment for Machine Learning
|journal=Neural Information Processing Systems
|year=2011
|url=http://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf
}}</ref> It provides a wide range of algorithms for [[deep learning]], and uses the scripting language [[LuaJIT]], and an underlying [[C (programming language)|C]] implementation. As of 2018, Torch is no longer in active development.<ref>[https://github.com/torch/torch7/blob/master/README.md Torch GitHub repository ReadMe]</ref>

== torch ==
The core package of Torch is <code>torch</code>. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like <code>max</code>, <code>min</code>, <code>sum</code>,  statistical distributions like [[Uniform distribution (continuous)|uniform]], [[Normal distribution|normal]] and [[Multinomial distribution|multinomial]], and [[BLAS]] operations like [[dot product]], [[matrix-vector multiplication]], [[matrix multiplication|matrix-matrix multiplication]], [[matrix-vector product]] and [[matrix product]].

The following exemplifies using torch via its [[REPL]] interpreter:
<syntaxhighlight lang="lua">
> a = torch.randn(3,4)

> =a
-0.2381 -0.3401 -1.7844 -0.2615
 0.1411  1.6249  0.1708  0.8299
-1.0434  2.2291  1.0525  0.8465
[torch.DoubleTensor of dimension 3x4]

> a[1][2]
-0.34010116549482
	
> a:narrow(1,1,2)
-0.2381 -0.3401 -1.7844 -0.2615
 0.1411  1.6249  0.1708  0.8299
[torch.DoubleTensor of dimension 2x4]

> a:index(1, torch.LongTensor{1,2})
-0.2381 -0.3401 -1.7844 -0.2615
 0.1411  1.6249  0.1708  0.8299
[torch.DoubleTensor of dimension 2x4]

> a:min()
-1.7844365427828	
</syntaxhighlight>

The torch package also simplifies [[object oriented programming]] and [[serialization]] by providing various convenience functions which are used throughout its packages. The <code>torch.class(classname, parentclass)</code> function can be used to create [[Factory method pattern|object factories]] ([[Class (computer programming)|classes]]). When the [[Constructor (object-oriented programming)|constructor]] is called, torch initializes and sets a Lua [[Lua (programming language)#Tables|table]] with the user-defined [[Lua (programming language)#Metatables|metatable]], which makes the table an [[Object (computer science)|object]].

Objects created with the torch factory can also be serialized, as long as they do not contain references to objects that cannot be serialized, such as Lua [[coroutine]]s, and Lua ''userdata''. However, ''userdata'' can be serialized if it is wrapped by a table (or metatable) that provides  <code>read()</code> and <code>write()</code> methods.

== nn ==
The nn package is used for building [[neural network]]s. It is divided into modular objects that share a common <code>Module</code> interface. Modules have a <code>forward()</code> and <code>backward()</code> method that allow them to [[Feedforward neural network|feedforward]] and [[backpropagation|backpropagate]], respectively. Modules can be joined together using module [[Composite pattern|composites]], like <code>Sequential</code>, <code>Parallel</code> and <code>Concat</code> to create complex task-tailored graphs. Simpler modules like <code>Linear</code>, <code>Tanh</code> and <code>Max</code> make up the basic component modules. This modular interface provides first-order [[Automatic differentiation|automatic gradient differentiation]]. What follows is an example use-case for building a [[multilayer perceptron]] using Modules:
<syntaxhighlight lang="lua">
> mlp = nn.Sequential()
> mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units
> mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function
> mlp:add( nn.Linear(25, 1) ) -- 1 output
> =mlp:forward(torch.randn(10))
-0.1815
[torch.Tensor of dimension 1]
</syntaxhighlight>

[[Loss function]]s are implemented as sub-classes of <code>Criterion</code>, which has a similar interface to <code>Module</code>. It also has <code>forward()</code> and <code>backward()</code> methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the [[Mean Squared Error]] criterion implemented in <code>MSECriterion</code> and the [[cross-entropy]] criterion implemented in <code>ClassNLLCriterion</code>. What follows is an example of a Lua function that can be iteratively called to train 
an <code>mlp</code> Module on input Tensor <code>x</code>, target Tensor <code>y</code> with a scalar <code>learningRate</code>:    
<syntaxhighlight lang="lua">
function gradUpdate(mlp,x,y,learningRate)
  local criterion = nn.ClassNLLCriterion()
  pred = mlp:forward(x)
  local err = criterion:forward(pred, y); 
  mlp:zeroGradParameters();
  local t = criterion:backward(pred, y);
  mlp:backward(x, t);
  mlp:updateParameters(learningRate);
end
</syntaxhighlight>

It also has <code>StochasticGradient</code> class for training a neural network using [[Stochastic gradient descent]], although the Optim package provides much more options in this respect, like momentum and weight decay [[Regularization (mathematics)|regularization]].

==Other packages==
Many packages other than the above official packages are used with Torch. These are listed in the torch cheatsheet. These extra packages provide a wide range of utilities such as parallelism, asynchronous input/output, image processing, and so on. They can be installed with [[LuaRocks]], the Lua package manager which is also included with the Torch distribution.

==Applications==
Torch is used by the Facebook [[AI]] Research Group,<ref>[http://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab.html KDnuggets Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab]</ref> [[IBM]],<ref>[https://news.ycombinator.com/item?id=7928738 Hacker News]</ref> [[Yandex]]<ref>[https://www.facebook.com/yann.lecun/posts/10152077631217143?comment_id=10152089275552143&offset=0&total_comments=6 Yann Lecun's Facebook Page]</ref> and the [[Idiap Research Institute]].<ref>[https://www.idiap.ch/scientific-research/resources/torch IDIAP Research Institute : Torch]</ref> Torch has been extended for use on [[Android (operating system)|Android]]<ref>[https://github.com/soumith/torch-android Torch-android GitHub repository]</ref> and [[iOS]].<ref>[https://github.com/clementfarabet/torch-ios Torch-ios GitHub repository]</ref> It has been used to build hardware implementations for data flows like those found in neural networks.<ref>[http://pub.clement.farabet.net/ecvw11.pdf NeuFlow: A Runtime Reconfigurable Dataflow Processor for Vision]</ref>

Facebook has released a set of extension modules as open source software.<ref>{{cite web |url=https://www.wired.com/2015/01/facebook-open-sources-trove-ai-tools/ |title=Facebook Open-Sources a Trove of AI Tools |website=[[Wired (website)|Wired]] |date=16 January 2015}}</ref>

==See also==
* [[Comparison of deep learning software]]
* [[PyTorch]]

==References==
{{reflist|30em}}

==External links==
* {{Official website|http://torch.ch}}

{{Deep Learning Software}}
{{Lua programming language}}

[[Category:Data mining and machine learning software]]
[[Category:Deep learning]]
[[Category:Free statistical software]]
[[Category:Lua software]]
[[Category:Software using the BSD license]]
